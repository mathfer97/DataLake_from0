During this project we will learn to:
- Build a complete Data Lake
- Develop data pipelines in practice
- Create a full data infrastructure on AWS
- Use AWS tools such as S3, EMR, Databricks, Apache Airflow, Glue, Athena, and Metabase
- Understand the role of a Data Engineer in the market
- Comprehend the concept of a Data Lake and how to plan your data project
- Create an AWS account and configure local credentials
- Create an account and cluster on Databricks
- Launch an Apache Airflow instance locally and on AWS (MWAA)
- Create an S3 bucket to store your data
- Create a catalog in AWS Glue and query data using Athena
- Deploy a Metabase instance on AWS
- Connect Metabase to your Data Lake to build professional dashboards and charts

Tools: AWS, Python, PostgreSQL, Apache Airflow

First step:
Create an account on aws, install docker and an instance for apache airflow, i did this step following these tutorial below:
https://marcelo-albuquerque.medium.com/como-instalar-o-wsl-2-no-windows-10-3e26d99d7161
https://github.com/aws/aws-mwaa-local-runner?tab=readme-ov-file#prerequisites
